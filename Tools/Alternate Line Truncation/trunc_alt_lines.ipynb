{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOq7B_40lDK9",
        "outputId": "c3358e90-cbc2-4d7b-e3ff-c9dc4a377c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLXleXuvNmOV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Ni-8Uqswow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9be1ea2-3f6b-4e4d-f29a-dd1bfec6988e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaUFAGkoOjw9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeHFP-aKe8yS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEtV-2-RNpO5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQJ_1ZHyvhne"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS3kzHl6Mwkz"
      },
      "source": [
        "We will use weights and biases for tracking experiments and runs. Project page : https://wandb.ai/tasmiah-tahsin/fake-news-blurr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVEvLd64veua"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q3yba0FOm4D"
      },
      "outputs": [],
      "source": [
        "# fake = pd.read_csv(\"/content/drive/MyDrive/datasets/Fake-1K.csv\")\n",
        "# fake_syn = pd.read_csv(\"/content/drive/MyDrive/datasets/fake_synthetic.csv\")\n",
        "# authentic = pd.read_csv(\"/content/drive/MyDrive/datasets/Authentic-48K.csv\",engine='python',error_bad_lines=False,warn_bad_lines=True,nrows=15000)\n",
        "# df = pd.concat([authentic[['headline','content','label']],fake[['headline','content','label']],fake_syn[['headline','content','label']]])\n",
        "# df.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split1 = pd.read_csv(\"/content/drive/MyDrive/datasets/test.csv\")\n",
        "split1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAojrRaKdkFQ",
        "outputId": "18446149-a9de-491e-fd2e-c7405fe388e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preparing a new dataframe "
      ],
      "metadata": {
        "id": "RnyD-tE_2RP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sel_df = pd.DataFrame(columns = ['label', 'text', 'sum_text'])\n",
        "# final_df['sum_text'] = \"\""
      ],
      "metadata": {
        "id": "oSYBjTrE2ZT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_df.shape, sel_df.shape"
      ],
      "metadata": {
        "id": "9sylRssV4S0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code for Alt line Truncation"
      ],
      "metadata": {
        "id": "3_xxVY9_eSde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_len_wtokenizer(sent):\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    length = len(input_ids)\n",
        "    return length"
      ],
      "metadata": {
        "id": "fULL8SDyeXUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_len(sent):\n",
        "    word_count = len(sent.split())\n",
        "    return word_count"
      ],
      "metadata": {
        "id": "T58KqEJyebtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trunc_alt_sent(str_w):\n",
        "    x = str_w.rsplit(\"ред \")\n",
        "    n = len(x)\n",
        "    if n==1:\n",
        "      x = str_w.rsplit(\"ред\")\n",
        "      n = len(x)\n",
        "    res_str = \"default: \"\n",
        "    l2 = []\n",
        "    for i in range(0,n):\n",
        "        if(i%2==0):\n",
        "            #print(x[i])\n",
        "            l2.append(x[i])\n",
        "\n",
        "    res_str = \"ред \".join(l2)\n",
        "\n",
        "    #print(res_str)\n",
        "    return res_str\n",
        "    "
      ],
      "metadata": {
        "id": "_jnJOkxPef5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#timeout = time.time() + 60   # 1 minutes from now"
      ],
      "metadata": {
        "id": "Pu7pwYVCewON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# timeout = time.time() + 30\n",
        "# while True:    \n",
        "#     if time.time() > timeout:\n",
        "#       print('timeout')\n",
        "#       break"
      ],
      "metadata": {
        "id": "JUBeGkMde8h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itr=0\n",
        "sum_count=0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in split1['text']:   \n",
        "    \n",
        "    timeout = time.time() + 30\n",
        "    while(1):\n",
        "        if(calc_len(sent)<512):\n",
        "            break\n",
        "        \n",
        "        if(itr!=903):\n",
        "          sum_count+=1\n",
        "        \n",
        "        if(time.time() > timeout):\n",
        "            print('timeout. eror in row: ', itr)\n",
        "            break\n",
        "        sent = trunc_alt_sent(sent)\n",
        "        #leng = calc_len(sent)\n",
        "    \n",
        "    split1.at[itr,'text'] = sent\n",
        "    \n",
        "    if(itr%300==0):\n",
        "      print(itr)\n",
        "    itr+=1\n",
        "print(\"successfully done. sum count: \", sum_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbypk_GqgVAE",
        "outputId": "51fc5518-709a-408e-917a-a76d9e680ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "300\n",
            "600\n",
            "900\n",
            "timeout. eror in row:  903\n",
            "successfully done. sum count:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split1.to_csv('/content/drive/MyDrive/datasets/work2.csv', index=False)"
      ],
      "metadata": {
        "id": "Ix7sSdpfJXWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_hypo = split1\n",
        "testing_hypo.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGW5j-x3kBTW",
        "outputId": "17919788-f10d-4a17-bfad-5f9fba845e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10016, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_string = testing_hypo.iloc[0]['text']\n",
        "sent_t = trunc_alt_sent(a_string)\n",
        "testing_hypo.iloc[0]['text'] = sent_t\n",
        "testing_hypo.iloc[0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "wikZ_12WkJZv",
        "outputId": "c63e8a8f-0a2a-499a-eaf4-f2f4dbfedc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'рж╣ржЯрзНржЯржЧрзЛрж▓ ржХрж░рж╛рзЯ ржмрж╛ржХрзГржмрж┐рждрзЗ ржжрзБржЗржЬржи ржмрж░ржЦрж╛рж╕рзНржд, рзм ржЬржиржХрзЗ рж╢рзЛржХржЬржЧржд рззрзн рж╕рзЗржкрзНржЯрзЗржорзНржмрж░ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢ ржХрзГрж╖рж┐ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯрзЗ (ржмрж╛ржХрзГржмрж┐) ржЙржкрж╛ржЪрж╛рж░рзНржпрзЗрж░ ржХрж╛рж░рзНржпрж╛рж▓рзЯрзЗ рж╣ржЯрзНржЯржЧрзЛрж▓рзЗрж░ ржШржЯржирж╛рзЯ ржжрзБржЗржЬржиржХрзЗ рж╕рж╛ржорзЯрж┐ржХ ржмрж░ржЦрж╛рж╕рзНржд ржУ ржЫрзЯ ржЬржиржХрзЗ рж╢рзЛржХржЬ ржХрж░рзЗржЫрзЗ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯ ржкрзНрж░рж╢рж╛рж╕ржиред ржмрзБржзржмрж╛рж░ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯ ржмрж╛ржХрзГржмрж┐ рж░рзЗржЬрж┐рж╕рзНржЯрзНрж░рж╛рж░ рж╕рж╛ржЗржлрзБрж▓ ржЗрж╕рж▓рж╛ржо рж╕рзНржмрж╛ржХрзНрж╖рж░рж┐ржд ржПржХ ржирзЛржЯрж┐рж╢рзЗ ржЖржЧрж╛ржорзА рзн ржжрж┐ржирзЗрж░ ржоржзрзНржпрзЗ ржЙржкржпрзБржХрзНржд ржЙрждрзНрждрж░ ржжрзЗрзЯрж╛рж░ ржирж┐рж░рзНржжрзЗрж╢ ржжрзЗрзЯрж╛ рж╣рзЯрзЗржЫрзЗред ржПржжрж┐ржХрзЗ ржП ржШржЯржирж╛рзЯ ржЖржирзНржжрзЛрж▓ржирзЗрж░ рж╕ржЩрзНржЧрзЗ ржПржХрж╛рждрзНржмрждрж╛ ржкрзНрж░ржХрж╛рж╢ ржирж╛ ржХрж░рж╛рзЯ рж╣рж╛ржорж▓рж╛рж░ рж╢рж┐ржХрж╛рж░ рж╣рзЯрзЗ ржХрж╛рж░рж┐ржЧрж░рж┐ ржХрж░рзНржоржЪрж╛рж░рзА ржкрж░рж┐рж╖ржжрзЗрж░ рж╕ржнрж╛ржкрждрж┐ ржУ рж╕рж╛ржзрж╛рж░ржг рж╕ржорзНржкрж╛ржжржХ рж╣рж╛рж╕ржкрж╛рждрж╛рж▓рзЗ ржнрж░рзНрждрж┐ рж╣рзЯрзЗржЫрзЗржиред рж╕рж╛ржорзЯрж┐ржХ ржмрж░ржЦрж╛рж╕рзНрждрж░рж╛ рж╣рж▓рзЗржи- рж╢рж┐ржХрзНрж╖рж╛ ржмрж┐рж╖рзЯржХ рж╢рж╛ржЦрж╛рж░ ржХрж░рзНржоржЪрж╛рж░рзА ржУ рзйрзЯ рж╢рзНрж░рзЗржгрж┐рж░ рж╕рж╛ржзрж╛рж░ржг рж╕ржорзНржкрж╛ржжржХ ржорзЛ. ржорзЛрж╢рж╛рж░ржл рж╣рзЛрж╕рзЗржи ржУ ржХрж░рзНржоржХрж░рзНрждрж╛ ржкрж░рж┐рж╖ржжрзЗрж░ ржпрзБржЧрзНржо рж╕ржорзНржкрж╛ржжржХ ржЬрж┐рзЯрж╛ржЙрж░ рж░рж╣ржорж╛ржи ржЯрж┐ржЯрзБред ржПржЫрж╛рзЬрж╛ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯ рж╕ржорзНржкрзНрж░рж╕рж╛рж░ржг ржХрзЗржирзНржжрзНрж░рзЗрж░ рж╕рж╣ржХрж╛рж░рзА ржкрж░рж┐ржЪрж╛рж▓ржХ ржорзЛрж╣рж╛ржорзНржоржж ржЖржмрзБрж▓ ржмрж╛рж╕рж╛рж░ ржЖржоржЬрж╛ржж, ржбрзЗржкрзБржЯрж┐ рж▓рж╛ржЗржмрзНрж░рзЗрж░рж┐рзЯрж╛ржи ржорзЛ.ржЦрж╛ржЗрж░рзБрж▓ ржЖрж▓ржо ржирж╛ржирзНржирзБ, ржорзЛ.ржЖржмржжрзБрж▓ ржмрж╛рждрзЗржи, ржХрзНрж░рзАрзЬрж╛ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржмрж┐ржнрж╛ржЧрзЗрж░ ржорзЛрж╣рж╛ржорзНржоржж ржорзЛрж╕рзНрждрж╛ржЗржи ржХржмрзАрж░ рж╕рзЛрж╣рзЗрж▓, рж╕ржВрж╕рзНржерж╛ржкржи рж╢рж╛ржЦрж╛рж░ рж╕рж╣ржХрж╛рж░рзА рж░рзЗржЬрж┐рж╕рзНржЯрзНрж░рж╛рж░ ржорзЛрж╣рж╛ржорзНржоржж ржЖрж╢рж┐ржХрзБрж▓ ржЖрж▓ржо ржмрж╛ржЪрзНржЪрзБ ржУ ржЦрж╛ржорж╛рж░ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛ рж╢рж╛ржЦрж╛рж░ ржЕрзНржпрж╛ржбрж┐рж╢ржирж╛рж▓ рж░рзЗржЬрж┐рж╕рзНржЯрзНрж░рж╛рж░ ржб. ржорзЛ. рж╣рзЗрж▓рж╛рж▓ ржЙржжрзНржжрзАржиржХрзЗ ржХрж╛рж░ржг ржжрж░рзНрж╢рж╛ржирзЛрж░ ржирзЛржЯрж┐рж╢ ржжрзЗрзЯ ржкрзНрж░рж╢рж╛рж╕ржиред  ржирзЛржЯрж┐рж╢рзЗ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛ рж╣рзЯ, ржЧржд рж╕рзЛржоржмрж╛рж░ ржжрзБржкрзБрж░рзЗ рж╕рзЛрзЯрж╛ рззрзиржЯрж╛рж░ ржжрж┐ржХрзЗ ржЙржкрж╛ржЪрж╛рж░рзНржпрзЗрж░ ржЕржирзБржорждрж┐ ржЫрж╛рзЬрж╛ржЗ рж╣ржарж╛рзО ржХрж░рзЗ ржЙржкрж╛ржЪрж╛рж░рзНржпрзЗрж░ ржХрж╛рж░рзНржпрж╛рж▓рзЯрзЗ ржкрзНрж░ржмрзЗрж╢ ржХрж░рзЗ ржЫрж╛рждрзНрж░ ржмрж┐рж╖рзЯржХ ржЙржкржжрзЗрж╖рзНржЯрж╛, ржкрзНрж░ржХрзНржЯрж░, ржбрж┐ржи ржХрж╛ржЙржирзНрж╕рж┐рж▓рзЗрж░ ржЖрж╣рзНржмрж╛рзЯржХ, рж░рзЗржЬрж┐рж╕рзНржЯрзНрж░рж╛рж░ ржУ рж╕рж╛ржВржмрж╛ржжрж┐ржХржжрзЗрж░ рж╕рж╛ржоржирзЗ ржЙржкрж╛ржЪрж╛рж░рзНржп ржУ ржЙржк-ржЙржкрж╛ржЪрж╛рж░рзНржпржХрзЗ рж▓ржХрзНрж╖рзНржп ржХрж░рзЗ ржЖржЩрзНржЧрзБрж▓ ржЙржЪрж┐рзЯрзЗ ржХржЯрзБржХрзНрждрж┐ ржХрж░рзЗ ржПржмржВ ржЕрж╢рж╛рж▓рзАржи рж╢рж╛рж░рзАрж░рж┐ржХ ржЕржЩрзНржЧржнржЩрзНржЧрж┐ ржХрж░рзЗред ржПрждрзЗ ржХрж░рзЗ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯрзЗрж░ ржЙржЪрзНржЪ ржкрж░рзНржпрж╛рзЯрзЗрж░ ржкрзНрж░рж╛рж╢рж╛рж╕ржирж┐ржХ ржХрж╛рж░рзНржпржХрзНрж░ржо ржмрзНржпрж╛рж╣ржд рж╣рзЯ ржПржмржВ ржЙржкрж╛ржЪрж╛рж░рзНржпрзЗрж░ рж╕ржЩрзНржЧрзЗ ржжрзБрж░рзНржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржпрж╛ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯрзЗрж▓ ржЪрж╛ржХрж░рж┐ рж╕ржВржмрж┐ржзрж┐рж░ рж╕рзБрж╕рзНржкрж╖рзНржЯ рж▓ржЩрзНржШржи ржУ ржЧрзБрж░рзБрждрж░ ржЕржкрж░рж╛ржзред ржПржжрж┐ржХрзЗ ржкрзНрж░рж╢рж╛рж╕ржирзЗрж░ ржХрж╛рж░ржг ржжрж░рзНрж╢рж╛ржирзЛрж░ ржирзЛржЯрж┐рж╢ ржкрж╛ржУрзЯрж╛рж░ ржкрж░ ржЕржлрж┐рж╕рж╛рж░ ржкрж░рж┐рж╖ржжрзЗрж░ ржирзЗрждрж╛рж░рж╛ ржорж┐ржЫрж┐рж▓ ржирж┐рзЯрзЗ рж╣рж┐рж╕рж╛ржм рж╕ржВрж░ржХрзНрж╖ржг рж╢рж╛ржЦрж╛, ржкрзНрж░ржХрзМрж╢рж▓ рж╢рж╛ржЦрж╛, ржкрж░рж┐ржХрж▓рзНржкржирж╛ ржУ ржЙржирзНржирзЯржи рж╢рж╛ржЦрж╛рзЯ рждрж╛рж▓рж╛ ржЭрзБржБрж▓рж┐рзЯрзЗ ржжрзЗрзЯред рждржмрзЗ ржкрзНрж░рж╢рж╛рж╕ржи ржнржмржирзЗ ржкрзБрж▓рж┐рж╢ ржорзЛрждрж╛рзЯржи ржерж╛ржХрж╛рзЯ рждрж╛рж▓рж╛ ржжрж┐рждрзЗ ржмрзНржпрж░рзНрже рж╣рзЯред ржХрж░рзНржоржХрж░рзНрждрж╛ржжрзЗрж░ рж╕ржЩрзНржЧрзЗ ржПржХрж╛рждрзНржмрждрж╛ ржкрзНрж░ржХрж╛рж╢ ржХрж░рзЗ ржХрж░рзНржоржЪрж╛рж░рзАрж░рж╛ржУ ржХрзНржпрж╛ржорзНржкрж╛рж╕рзЗ ржорж┐ржЫрж┐рж▓ ржХрж░рзЗред ржПрж╕ржорзЯ рждрж╛рж░рж╛ ржкрзНрж░рж╢рж╛рж╕ржирзЗрж░ ржмрж┐рж░рзБржжрзНржзрзЗ ржмрж┐ржнрж┐ржирзНржи ржзрж░ржирзЗ рж╢рзНрж▓рзЛржЧрж╛ржи ржжрж┐рждрзЗ ржерж╛ржХрзЗред  ржПржжрж┐ржХрзЗ ржХрж╛рж░рж┐ржЧрж░рж┐ ржХрж░рзНржоржЪрж╛рж░рзА ржкрж░рж┐рж╖ржжрзЗрж░ ржкржХрзНрж╖ ржерзЗржХрзЗ ржЖржирзНржжрзЛрж▓ржирзЗ рж╕ржЩрзНржЧрзЗ ржПржХрж╛рждрзНржмрждрж╛ ржкрзНрж░ржХрж╛рж╢ ржирж╛ ржХрж░рж╛рзЯ рж╣рж╛ржорж▓рж╛ ржХрж░рзЗ рзйрзЯ ржУ ржЪрждрзБрж░рзНрже рж╢рзНрж░рзЗржгрж┐рж░ ржХрж░рзНржоржЪрж╛рж░рзАрж░рж╛ред ржПржмрж┐рж╖рзЯрзЗ ржХрж╛рж░рж┐ржЧрж░рж┐ ржХрж░рзНржоржЪрж╛рж░рзА ржкрж░рж┐рж╖ржжрзЗрж░ рж╕ржнрж╛ржкрждрж┐ ржорзЛ. ржЖржмржжрзБрж▓ ржорзЛрждрж╛рж▓рзЗржм ржмрж▓рзЗржи, ржЖржорж░рж╛ ржЪрж╛рж░ ржжржлрж╛ ржжрж╛ржмрж┐рждрзЗ ржРржХрзНржпржмржжрзНржз рж╣рзЯрзЗржЫрж┐рж▓рж╛ржоред ржЧржд рззрзн рж╕рзЗржкрзНржЯрзЗржорзНржмрж░ ржЖржорж╛ржжрзЗрж░ рж░рзЗржЦрзЗржЗ рзйрзЯ рж╢рзНрж░рзЗржгрж┐рж░ рж╕рж╛ржзрж╛рж░ржг рж╕ржорзНржкрж╛ржжржХ ржорзЛ. ржорзЛрж╢рж╛рж░ржл рж╣рзЛрж╕рзЗржи ржХрж░рзНржоржХрж░рзНрждрж╛ржжрзЗрж░ рж╕ржЩрзНржЧрзЗ ржнрж┐рж╕рж┐ рж╕ржЪрж┐ржмрж╛рж▓рзЯрзЗ ржврзБржХрзЗ ржнрж┐рж╕рж┐рж░ рж╕ржЩрзНржЧрзЗ ржмрзЗрзЯрж╛ржжржмрж┐ ржХрж░рзЗред ржкрж░ржмрж░рзНрждрзАрждрзЗ рждрж╛ржХрзЗ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓рзЯ ржерзЗржХрзЗ рж╢рзЛржХржЬ ржХрж░рж▓рзЗ ржЖржорж╛ржХрзЗ ржУ ржЖржорж╛рж░ рж╕ржВржЧржаржирзЗрж░ рж╕ржХрж▓ржХрзЗ рждрж╛ржжрзЗрж░ рж╕ржЩрзНржЧрзЗ ржЖржирзНржжрзЛрж▓ржирзЗ ржпрзЗрждрзЗ ржмрж▓рзЗред ржпрзЛржЧ ржирж╛ ржжрж┐рж▓рзЗ ржкрж░рзЗ рзйрзЯ ржУ рзкрж░рзНрже рж╢рзНрж░рзЗржгрж┐ ржорж┐рж▓рзЗ ржЖржорж╛ржжрзЗрж░ рж╕ржВржЧржаржирзЗ рж╣рж╛ржорж▓рж╛ ржХрж░рзЗ, ржЪрзЗрзЯрж╛рж░ ржнрж╛ржЩрзЗред ржП ржШржЯржирж╛рзЯ ржЖржорж┐ ржУ ржЖржорж╛рж░ рж╕ржВржЧржаржирзЗрж░ рж╕рж╛ржзрж╛рж░ржг рж╕ржорзНржкрж╛ржжржХ рж╕рзБрж▓рждрж╛ржи ржЖрж╣ржорзЗржж ржЖрж╣ржд рж╣рзЯрзЗ ржмрж░рзНрждржорж╛ржирзЗ ржорзЯржоржирж╕рж┐ржВрж╣ рж╣рж╛рж╕ржкрж╛рждрж╛рж▓рзЗ ржнрж░рзНрждрж┐ ржЖржЫрж┐ред ржорзЛ. рж╢рж╛рж╣рзАржи рж╕рж░ржжрж╛рж░/ржЖрж░ржП/ржЖрж░ржЖржЗржкрж┐'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code for Slicing df before summarization\n"
      ],
      "metadata": {
        "id": "QloXfBVVjlZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "K9-59f9Gfigh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba644af-2f7a-4f59-b144-1213e38f02ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/config.json\n",
            "Model config MT5Config {\n",
            "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_length\": 84,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"no_repeat_ngram_size\": 2,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/config.json\n",
            "Model config MT5Config {\n",
            "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_length\": 84,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"no_repeat_ngram_size\": 2,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/config.json\n",
            "Model config MT5Config {\n",
            "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_length\": 84,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"no_repeat_ngram_size\": 2,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/config.json\n",
            "Model config MT5Config {\n",
            "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_length\": 84,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"no_repeat_ngram_size\": 2,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--mT5_multilingual_XLSum/snapshots/2437a524effdbadc327ced84595508f1e32025b3/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at csebuetnlp/mT5_multilingual_XLSum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "i=0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in final_df['text']:   \n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Only transfering greater than 500 token_length\n",
        "    length = len(input_ids)\n",
        "    if(length>500):\n",
        "        sel_df = sel_df.append(final_df.iloc[i]) \n",
        "        final_df = final_df.drop(i)\n",
        "        counter+=1    \n",
        "    \n",
        "    i+=1              \n",
        "\n",
        "print('number of sum text: ', counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "omy9EJ9D3ixU",
        "outputId": "d9003f8d-9ee0-4438-b16a-b0665bc0d45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-63f26f0272de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msel_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sel_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfi-F1GR40Hk",
        "outputId": "e12ab58d-b14b-446e-f7fa-b211a3d81426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3931, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxV2151S45Ba",
        "outputId": "8338d7a9-5115-4ff0-de43-65f58a4ea9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6085, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sel_df.to_csv('/content/drive/MyDrive/datasets/selected_forsum.csv')\n",
        "final_df.to_csv('/content/drive/MyDrive/datasets/mod_final.csv')"
      ],
      "metadata": {
        "id": "R46kbMuZ5Qur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##slicing into 9 parts"
      ],
      "metadata": {
        "id": "xcMIOG-368aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split1 = sel_df.iloc[:440, 0:]\n",
        "split2 = sel_df.iloc[440:880, 0:]\n",
        "split3 = sel_df.iloc[880:1320, 0:]\n",
        "split4 = sel_df.iloc[1320:1760, 0:]\n",
        "split5 = sel_df.iloc[1760:2200, 0:]\n",
        "split6 = sel_df.iloc[2200:2640, 0:]\n",
        "split7 = sel_df.iloc[2640:3080, 0:]\n",
        "split8 = sel_df.iloc[3080:3520, 0:]\n",
        "split9 = sel_df.iloc[3520:3930, 0:]"
      ],
      "metadata": {
        "id": "sb4ZvtyN6_Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split9.shape, split8.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuozx2Ux9Lzy",
        "outputId": "754f1770-5449-455f-a399-37d8b19588d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((410, 3), (440, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split1.to_csv('split1.csv')\n",
        "split2.to_csv('split2.csv')\n",
        "split3.to_csv('split3.csv')\n",
        "split4.to_csv('split3.csv')\n",
        "split5.to_csv('split5.csv')\n",
        "split6.to_csv('split6.csv')\n",
        "split7.to_csv('split7.csv')\n",
        "split8.to_csv('split8.csv')\n",
        "split9.to_csv('split9.csv')\n"
      ],
      "metadata": {
        "id": "e5Qsrnwn9hpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split1.to_csv('/content/drive/MyDrive/datasets/split1.csv')"
      ],
      "metadata": {
        "id": "GhbTQIPk-mUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split2.to_csv('/content/drive/MyDrive/datasets/split2.csv')\n",
        "split3.to_csv('/content/drive/MyDrive/datasets/split3.csv')\n",
        "split4.to_csv('/content/drive/MyDrive/datasets/split4.csv')\n",
        "split5.to_csv('/content/drive/MyDrive/datasets/split5.csv')\n",
        "split6.to_csv('/content/drive/MyDrive/datasets/split6.csv')\n",
        "split7.to_csv('/content/drive/MyDrive/datasets/split7.csv')\n",
        "split8.to_csv('/content/drive/MyDrive/datasets/split8.csv')\n",
        "split9.to_csv('/content/drive/MyDrive/datasets/split9.csv')"
      ],
      "metadata": {
        "id": "7Szw_ZWS-9mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the csv"
      ],
      "metadata": {
        "id": "hvEsJ5LluQd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('sum_train.csv', index=False)\n",
        "path_tsum = '/content/drive/MyDrive/datasets/sum_train.csv'\n",
        "with open(path1, 'w') as f:\n",
        "  final_df.to_csv(f, index=False)"
      ],
      "metadata": {
        "id": "TlRxVcZHuT8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmaLJWyNB2U"
      },
      "source": [
        "# Resources\n",
        "\n",
        "* fastai paper : https://arxiv.org/pdf/2002.04688.pdf\n",
        "* [BERT Fine-Tuning Tutorial with PyTorch ┬╖ Chris McCormick](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "* [multilingual bert](https://huggingface.co/bert-base-multilingual-cased)\n",
        "* https://github.com/cdpierse/transformers-interpret\n",
        "* https://blog.dataiku.com/the-learning-rate-finder-technique-how-reliable-is-it"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SaUFAGkoOjw9",
        "XpmaLJWyNB2U"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}